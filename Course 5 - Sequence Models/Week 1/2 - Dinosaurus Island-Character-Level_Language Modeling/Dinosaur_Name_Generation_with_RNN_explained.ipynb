{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dinosaur Name Generation with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = open('dinos.txt', 'r').read()\n",
    "# data, type=str\n",
    "# Aachenosaurus\n",
    "# Aardonyx\n",
    "# Abdallahsaurus\n",
    "# Abelisaurus\n",
    "# ...\n",
    "# Zuoyunlong\n",
    "# Zupaysaurus\n",
    "# Zuul\n",
    "data = data.lower()\n",
    "chars = list(set(data)) # ['r', 'n', 'l', 'h', 'm', 'b', 'a', 'o', 's', 'k', 'w', 'x', 'd', 'c', 'g', 'u', 'q', 'j', 'e', 'p', 't', 'f', 'y', 'z', 'i', '\\n', 'v']\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo hai dict char_to_ix và ix_to_char\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) } \n",
    "# {'r': 0, 'n': 1, 'l': 2, 'h': 3, 'm': 4, 'b': 5, 'a': 6, 'o': 7, 's': 8, 'k': 9, 'w': 10, 'x': 11, 'd': 12, 'c': 13, 'g': 14, 'u': 15, 'q': 16, 'j': 17, 'e': 18, 'p': 19, 't': 20, 'f': 21, 'y': 22, 'z': 23, 'i': 24, '\\n': 25, 'v': 26}\n",
    "\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "# {0: 'r', 1: 'n', 2: 'l', 3: 'h', 4: 'm', 5: 'b', 6: 'a', 7: 'o', 8: 's', 9: 'k', 10: 'w', 11: 'x', 12: 'd', 13: 'c', 14: 'g', 15: 'u', 16: 'q', 17: 'j', 18: 'e', 19: 'p', 20: 't', 21: 'f', 22: 'y', 23: 'z', 24: 'i', 25: '\\n', 26: 'v'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các hàm sau đã có sẵn trong utils.py\n",
    "```python\n",
    "def rnn_forward(X, Y, a_prev, parameters):\n",
    "    \"\"\" Performs the forward propagation through the RNN and computes the cross-entropy loss.\n",
    "    It returns the loss' value as well as a \"cache\" storing values to be used in backpropagation.\"\"\"\n",
    "    ....\n",
    "    return loss, cache\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    \"\"\" Performs the backward propagation through time to compute the gradients of the loss with respect\n",
    "    to the parameters. It returns also all the hidden states.\"\"\"\n",
    "    ...\n",
    "    return gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\" Updates parameters using the Gradient Descent Update Rule.\"\"\"\n",
    "    ...\n",
    "    return parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue): # Hàm phụ để tối ưu tối hơn\n",
    "    gradients = copy.deepcopy(gradients)\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    # Clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)\n",
    "    for gradient in gradients:\n",
    "        np.clip(gradients[gradient], -maxValue, maxValue, out = gradients[gradient])\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    # Forward propagate through time (≈1 line)\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    # Backpropagate through time (≈1 line)\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    # Update parameters (≈1 line)\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]\n",
    "\n",
    "\n",
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- Python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    char_to_ix -- Python dictionary mapping each character to an index.\n",
    "    seed -- Used for grading purposes. Do not worry about it.\n",
    "\n",
    "    Returns:\n",
    "    indices -- A list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "\n",
    "    # Step 1: Create the a zero vector x that can be used as the one-hot vector \n",
    "    # Representing the first character (initializing the sequence generation). (≈1 line)\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    # Step 1': Initialize a_prev as zeros (≈1 line)\n",
    "    a_prev = np.zeros((n_a ,1))\n",
    "\n",
    "    # Create an empty list of indices. This is the list which will contain the list of indices of the characters to generate (≈1 line)\n",
    "    indices = []\n",
    "\n",
    "    # idx is the index of the one-hot vector x that is set to 1\n",
    "    # All other positions in x are zero.\n",
    "    # Initialize idx to -1\n",
    "    idx = -1\n",
    "\n",
    "    # Loop over time-steps t. At each time-step:\n",
    "    # Sample a character from a probability distribution \n",
    "    # And append its index (`idx`) to the list \"indices\". \n",
    "    # You'll stop if you reach 50 characters \n",
    "    # (which should be very unlikely with a well-trained model).\n",
    "    # Setting the maximum number of characters helps with debugging and prevents infinite loops. \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "\n",
    "        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
    "        a = np.tanh(np.dot(Wax,x) + np.dot(Waa,a_prev) + b)\n",
    "        z = np.dot(Wya,a) + by\n",
    "        y = softmax(z)\n",
    "\n",
    "        # For grading purposes\n",
    "        np.random.seed(counter + seed) \n",
    "\n",
    "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        # (see additional hints above)\n",
    "        idx = np.random.choice(range(len(y)), p = np.squeeze(y))\n",
    "\n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "\n",
    "        # Step 4: Overwrite the input x with one that corresponds to the sampled index `idx`.\n",
    "        # (see additional hints above)\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[idx] = 1\n",
    "\n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "\n",
    "        # for grading purposes\n",
    "        seed += 1\n",
    "\n",
    "        counter +=1\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model\n",
    "\n",
    "<img src=\"images/rnn.png\" height=350/>\n",
    "<img src=\"images/dinos3.png\" height=350/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data_x, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27, verbose = False):    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size # n_x=n_y=27\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    # parameters={\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
    "    # Wax shape=(n_a, n_x)=(50, 27), Waa shape=(n_a, n_a)=(50, 50), W_ya shape=(n_y, n_a)=(27, 50), b shape=(n_a, 1)=(50, 1), b_y shape=(n_y, 1)=(27, 1).\n",
    "\n",
    "    # Initialize loss (this is required because we want to smooth our loss)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    examples = [x.strip() for x in data_x] # len=1536, =['aachenosaurus', 'aardonyx', 'abdallahsaurus', 'abelisaurus', ...]\n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    # examples=['turiasaurus', 'pandoravenator', 'ilokelesia', ...]\n",
    "    \n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # for grading purposes\n",
    "    last_dino_name = \"abc\"\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations): # Trong một vòng lặp\n",
    "        # 1. Chọn random một name từ dataset để train bằng cách Lấy index của current example, lấy name tương ứng, rồi chuyển name đó thành list of indices.\n",
    "\n",
    "        # Set the index `idx` (see instructions above) \n",
    "        idx = j%len(examples) # e.g.: j=0, idx=0\n",
    "        \n",
    "        # Set the input X (see instructions above)\n",
    "        single_example_chars = examples[idx] # single_example_chars=turiasaurus\n",
    "        single_example_ix = [char_to_ix[c] for c in single_example_chars] # single_example_ix=[20, 15, 0, 24, 6, 8, 6, 15, 0, 15, 8]\n",
    "\n",
    "        # 2. Khởi tạo x^{<1>}=None. Tính training example X bằng cách ghép x^{<1>} với single_example_ix. Tính label Y là X dịch forward đi một và thêm index của `\\n`.\n",
    "        # Ta cần X=None ở đầu để sau đó Y dài bằng X\n",
    "\n",
    "        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. \n",
    "        X = [None] + single_example_ix # X=[None, 20, 15, 0, 24, 6, 8, 6, 15, 0, 15, 8], len=12\n",
    "        \n",
    "        # Set the labels Y (see instructions above)\n",
    "        # The goal is to train the RNN to predict the next letter in the name\n",
    "        # So the labels are the list of characters that are one time-step ahead of the characters in the input X\n",
    "        Y = X[1:] # Y=[20, 15, 0, 24, 6, 8, 6, 15, 0, 15, 8], len=11\n",
    "        # The RNN should predict a newline at the last letter, so add ix_newline to the end of the labels\n",
    "        ix_newline = [char_to_ix[\"\\n\"]]\n",
    "        Y = Y + ix_newline # Y=[20, 15, 0, 24, 6, 8, 6, 15, 0, 15, 8, 25], len=12\n",
    "\n",
    "        # Trong hàm optimize\n",
    "        # 3. Truyền X và Y vào RNN. Trong hàm rnn_forward\n",
    "        # - Khởi tạo a0=a_prev=0, có shape (n_a, 1). Đặt a[-1]=a0\n",
    "        # - Khởi tạo các dicts x, a, y_hat = {}, {}, {}\n",
    "        # - Lần lượt lặp qua các time steps t. Tại một time step t:\n",
    "        #     - Tính x[t] là một one-hot vector, có shape (n_x, 1), có giá trị $1$ tại vị trí Xt.\n",
    "        #       VD tại t=1, X[t]=20, x[t]=[[0], [0], ..., [0], [1], [0]..., [0]], có 1 tại vị trí 20\n",
    "        #     - Truyền a[t-1] (tại t=0 là a0) và x[t] vào một RNN cell, từ đó tính a[t] và y_hat[t].\n",
    "        #     y_hat[t] có shape (n_y, 1)=(27, 1), là một probability vector, \n",
    "        #     giá trị tại vị trí i thể hiện XS character có index i sẽ là next character.\n",
    "        #     - Tính loss là negative log của giá trị y_hat[t] tại vị trí Y[t]:      \n",
    "        #         loss = -log(y_hat[t][Y[t]]).\n",
    "        # Sau khi lặp xong, dict x có len 12, dict a có len 13, dict y_hat có len 12\n",
    "\n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "        # Lấy ra loss và a_prev chính là a[t] cuối\n",
    "        \n",
    "        \n",
    "        # debug statements to aid in correctly forming X, Y\n",
    "        if verbose and j in [0, len(examples) -1, len(examples)]:\n",
    "            print(\"j = \" , j, \"idx = \", idx,) \n",
    "        if verbose and j in [0]:\n",
    "            #print(\"single_example =\", single_example)\n",
    "            print(\"single_example_chars\", single_example_chars)\n",
    "            print(\"single_example_ix\", single_example_ix)\n",
    "            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n",
    "        \n",
    "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # d. Generate text (có thể gen sau mỗi n iterations). Lúc này model đã đc trained và đc fixed.\n",
    "        # Every 1000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 1000 == 0:\n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            # Sample `dino_names` lần để có `dino_names` generated names. Trong một lần sample:\n",
    "            # The number of dinosaur names to print\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                # Trong hàm sample:\n",
    "                # - Khởi tạo x=0, có shape (n_x, 1) và a_prev=0, có shape (n_a, 1).\n",
    "                # - Dùng while để lặp khi chưa gen ra kí tự \"\\n\"\n",
    "                # - Chạy một forward propagation để tính a^{<1>} và \\hat{y}^{<1>}.\n",
    "                # - Nếu chọn character có XS cao nhất, model sẽ luôn cho ra cùng KQ cho một starting letter.\n",
    "                #   Để có nhiều KQ hơn, ta chọn next letter ngẫu nhiên dựa trên XS.\n",
    "                #   Tạo idx là id của letter đc chọn\n",
    "                # - Cập nhật x^{<t>} thành x^{<t+1>} bằng cách tạo x là one-hot vector có vị trí 1 tại idx.\n",
    "                # - Truyền $x^{<t+1>}$ vào step 1 để lặp lại process tới khi có character `\\n`.\n",
    "                # - Output của hàm `sample` là list các idx đã tính, sẽ đc chuyển thành name tương ứng.\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                last_dino_name = get_sample(sampled_indices, ix_to_char)\n",
    "                print(last_dino_name.replace('\\n', ''))\n",
    "                \n",
    "                seed += 1  # To get the same result (for grading purposes), increment the seed by one. \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters, last_dino_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j =  0 idx =  0\n",
      "single_example_chars turiasaurus\n",
      "single_example_ix [20, 15, 0, 24, 6, 8, 6, 15, 0, 15, 8]\n",
      " X =  [None, 20, 15, 0, 24, 6, 8, 6, 15, 0, 15, 8] \n",
      " Y =        [20, 15, 0, 24, 6, 8, 6, 15, 0, 15, 8, 25] \n",
      "\n",
      "Iteration: 0, Loss: 23.087337\n",
      "\n",
      "Gxviztmcajub\n",
      "Xgalrmtyolnczxzoad\n",
      "Xviztmcajub\n",
      "Galrmtyolnczxzoad\n",
      "Viztmcajub\n",
      "Alrmtyolnczxzoad\n",
      "Iztmcajub\n",
      "\n",
      "\n",
      "Iteration: 1000, Loss: 28.769617\n",
      "\n",
      "Koitiqhonusayhus\n",
      "Osaaraetannopatamoyoll\n",
      "Oitiqhonusayhus\n",
      "Kanraetanrseus\n",
      "\n",
      "Bnraetanrseus\n",
      "Itenolusayhus\n",
      "\n",
      "\n",
      "j =  1535 idx =  1535\n",
      "j =  1536 idx =  0\n",
      "Iteration: 2000, Loss: 27.868678\n",
      "\n",
      "Ksitpurus\n",
      "Osasrus\n",
      "Ottesaurus\n",
      "Karraeunmhopapnos\n",
      "\n",
      "Anrausanrustplasiurothostmripkhonagopsaurus\n",
      "Ytersaurus\n",
      "\n",
      "\n",
      "Iteration: 3000, Loss: 26.795501\n",
      "\n",
      "Saypichongga\n",
      "Asadragtanlops\n",
      "Otipclos\n",
      "Sanraucarns\n",
      "\n",
      "Mhnaceransus\n",
      "Zichongda\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.849603\n",
      "\n",
      "Sayceongosaurus\n",
      "Ablaratensmugoenos\n",
      "A\n",
      "Sanraetangdilimos\n",
      "Iteoniosaulus\n",
      "Mmrajianhusteras\n",
      "Yperosaurus\n",
      "\n",
      "\n",
      "Iteration: 5000, Loss: 25.277315\n",
      "\n",
      "Kuzytersaurus\n",
      "Scharateran\n",
      "S\n",
      "Wannateran\n",
      "Itterosaurus\n",
      "Blnocenangys\n",
      "Yterosaurus\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.619292\n",
      "\n",
      "Sateptosaurus\n",
      "Scharaptoros\n",
      "S\n",
      "Sanraptoraceltonx\n",
      "Ieeenosaurus\n",
      "Marbedonosis\n",
      "Tesaurus\n",
      "\n",
      "\n",
      "Iteration: 7000, Loss: 24.475120\n",
      "\n",
      "Kuittsaurus\n",
      "Osaurus\n",
      "Sitigasaurus\n",
      "Korraptorauumimoseuravalusaurus\n",
      "Itygasaurus\n",
      "Blnateralops\n",
      "Yterosaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.113237\n",
      "\n",
      "Kotypurosaurus\n",
      "Sonlnajtoras\n",
      "Sititan\n",
      "Karraptorostophoseuruchoserlipsaurus\n",
      "Itesaurus\n",
      "Marheganaus\n",
      "Ttonchusaurus\n",
      "\n",
      "\n",
      "Iteration: 9000, Loss: 23.745953\n",
      "\n",
      "Wgititan\n",
      "Sennratibanttosaurus\n",
      "Sijignoater\n",
      "Warratilang\n",
      "Iettaolesaurus\n",
      "Maraptor\n",
      "Titholesaurus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.778236\n",
      "\n",
      "Saptelosaurus\n",
      "Osaurus\n",
      "Oitesaurus\n",
      "Salrateranops\n",
      "Iugkasaurus\n",
      "Marateranops\n",
      "Teorolosaurus\n",
      "\n",
      "\n",
      "Iteration: 11000, Loss: 23.466176\n",
      "\n",
      "Dotieuroaedria\n",
      "Sauralus\n",
      "Siptoneltoa\n",
      "Dannatia\n",
      "Ieepbomeomia\n",
      "Brratia\n",
      "Typholus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters, last_name = model(data.split(\"\\n\"), ix_to_char, char_to_ix, num_iterations = 11001, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* Ở đây tại mỗi vòng lặp training ta chỉ lấy ra một single name thay vì một batch, nên một sample X sẽ có shape $(n_x, T_x)$. Ở đây $n_x=27$ còn $T_x$ thay đổi theo sample đc chọn.\n",
    "* Khi implement thực tế, ta đã tạo X là một list các indices có len $T_x$ và tạo x là một dict có len $T_x$, mỗi element x[t] có shape $(n_x, 1)$ là one-hot vector, có giá trị $1$ tại index X[t]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
