{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rnn_utils import *\n",
    "from public_tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Forward Propagation for the Basic RNN\n",
    "* Xét VD RNN có $T_x=T_y$\n",
    "\n",
    "  ![](images/RNN.png)\n",
    "\n",
    "* Input $x$:\n",
    "    * Với một single time step, $x^{(i)(t)}$ là $i^{th}$ sample tại time step $t$, là một 1D vector. VD với một 5000-word vocabulary, một word có thể đc biểu diễn bằng một 5000D vector.\n",
    "    * $n_x$ là số lượng units (nghĩa là số features) trong một single time step của một training example.\n",
    "* Time steps\n",
    "    * Một RNN có nhiều time step, đc đánh chỉ số bằng $t$.\n",
    "    * $T_x$ là số lượng time steps trong longest sequence.\n",
    "* Một mini-batch có shape là $\\left(n_x, m, T_x\\right)$, với $m$ là batch size. \n",
    "* Khác với data ảnh khi ta lặp theo sample, mỗi sample có size (h, w), đối với NLP ta lặp theo time step. Tại một time step, ta dùng một 2D slice có shape $\\left(n_x, m\\right)$, chính là $x^{<t>}$\n",
    "* Hidden state $a$:\n",
    "    * Hidden state của một single training example là một 1D vector dài $n_a$.\n",
    "    * Shape của hidden state cho một mini-batch là $\\left(n_a, m\\right)$, chính là $a^{<t>}$.\n",
    "    * Nếu tính cả time step thì shape là $\\left(n_a, m, T_x\\right)$\n",
    "* Prediction $\\hat{y}$:\n",
    "    * Là một 3D tensor có shape $\\left(n_y, m, T_y\\right)$, với $n_y$ là số lượng units trong vector biểu diễn prediction (VD là các XS để chọn kí tự), $T_y$ là số lượng time steps trong prediction.\n",
    "    * Với một single time step, $\\hat{y}^{<t>}$ có shape $\\left(n_y, m\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. RNN Cell\n",
    "* RNN cell trong một single time step là: \n",
    "\n",
    "  ![](images/rnn_step_forward_figure2_v3a.png)\n",
    "    \n",
    "* Một RNN cell có input là current input $x^{<t>}$ và previous hidden state $a^{<t-1>}$ (chứa thông tin từ QK), và output ra $a^{<t>}$ để đưa vào RNN cell tiếp theo và cũng để dự đoán $\\hat{y}^{<t>}$.\n",
    "* Tính hidden state:   \n",
    "    $$\n",
    "    a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)\n",
    "    $$\n",
    "    $W_{aa}$ có shape $\\left(n_a, n_a\\right)$, $W_{ax}$ có shape $\\left(n_a, n_x\\right)$, $b_a$ có shape $\\left(n_a, 1\\right)$.\n",
    "    \n",
    "* Tính prediction:  \n",
    "    $$\n",
    "    \\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)\n",
    "    $$\n",
    "    $W_{ya}$ có shape $\\left(n_y, n_a\\right)$, $b_y$ có shape $\\left(n_y, 1\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "   \n",
    "    # compute next activation state using the formula given above\n",
    "    q=np.dot(Waa,a_prev) + np.dot(Wax,xt) + ba\n",
    "    a_next = np.tanh(np.dot(Waa,a_prev) + np.dot(Wax,xt) + ba)\n",
    "\n",
    "    # compute output of the current cell using the formula given above\n",
    "    yt_pred = softmax( np.dot(Wya,a_next) + by)\n",
    "    \n",
    "    # store values you need for backward propagation in cache\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. RNN Forward Pass\n",
    "Dùng lại RNN cell theo time step, mỗi lần dùng cùng các parameters $W_{ax}, W_{aa}, b_a, W_{ya}, b_y$ .   \n",
    "  \n",
    "  ![](images//rnn_forward_sequence_figure3_v3a.png)\n",
    "    \n",
    "Các bước thực hiện:\n",
    "  1. Khởi tạo một ma trận 3D $a$ bằng 0, có shape $\\left(n_a, m, T_x\\right)$, để lưu tất cả hidden states.\n",
    "  2. Khởi tạo một ma trận 3D $\\hat{y}$ bằng 0, có shape $\\left(n_y, m, T_y\\right)$, để lưu các predictions.\n",
    "  3. Khởi tạo $a^{<0>}$ random, có shape $\\left(n_a, m\\right)$, và đặt $a^{<t>}=a^{<0>}$ (là `a_next` trong code).\n",
    "  4. Tại mỗi time step $t$:\n",
    "      - Lấy $x^{<t>}$ là một 2D slice of $x$ tại time step $t$, có shape $\\left(n_x, m\\right)$\n",
    "      - Chạy `rnn_cell_forward` để cập nhật $a^{<t>}$ và tính $\\hat{y}^{<t>}$.\n",
    "      - Lưu $a^{<t>}$ vào $a$ tại vị trí $t$.\n",
    "      - Lưu $\\hat{y}^{<t>}$ vào $\\hat{y}$ tại vị trí $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, a0, parameters):\n",
    "    # Initialize \"caches\" which will contain the list of all caches\n",
    "    caches = []\n",
    "    \n",
    "    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "    \n",
    "    # initialize \"a\" and \"y_pred\" with zeros (≈2 lines)\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next (≈1 line)\n",
    "    a_next = a0\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, compute the prediction, get the cache (≈1 line)\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the prediction in y (≈1 line)\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        # Append \"cache\" to \"caches\" (≈1 line)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "* RNN có vấn đề vanishing gradients.\n",
    "* RNN hoạt động tốt nhất khi mỗi output $\\hat{y}^{<t>}$ có thể đc ước lượng bằng local context. Local context chỉ các information gần với time step $t$ tại prediction, cụ thể là các inputs $x^{<t'>}$ với $t'$ gần $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM\n",
    "### 2.1. LSTM Cell\n",
    "Một LSTM cell có cấu trúc:\n",
    "   \n",
    "![](images/LSTM_figure4_v3a.png)\n",
    "    \n",
    "Một LSTM cell theo dõi và update một cell state, hay memory variable $c^{<t>}$ tại mọi time step, khác với $a^{<t>}$.\n",
    "\n",
    "### Forget gate $\\Gamma_f$\n",
    "Là một tensor chứa các values từ $0$ đến $1$. Nếu một unit trong forget gate có giá trị gần $0$, LSTM sẽ *quên* stored state trong unit tương ứng của previous cell state. Nếu gần $1$ thì LSTM sẽ gần như nhớ giá trị tương ứng trong stored state.\n",
    "$$\n",
    "\\mathbf{\\Gamma}_f^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_f[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_f)\n",
    "$$\n",
    "$\\mathbf{W}_f$ có shape $\\left(n_a, n_a+n_x\\right)$, $\\mathbf{b}_f$ có shape $\\left(n_a, 1\\right)$, $\\Gamma_f$ có shape $\\left(n_a, m\\right)$, giống với shape của previous cell state $c^{<t-1>}$, để hai cái có thể nhân element-wise. \\\n",
    "$\\mathbf{W}_f$ kiểm soát forget gate’s behavior. Nhân $\\Gamma_f^{<t>}\n",
    "*c^{<t>}$ giống như áp dụng một mask vào previous cell state. Nếu một giá trị trong $\\Gamma_f^{<t>}$ gần $0$, thì tích sẽ gần $0$, khiến cho information đc lưu trong unit tương ứng ở $c^{<t>}$ ko đc nhớ cho next time step. Tương tự khi gần $1$, information sẽ đc dùng ở next time step\n",
    "    \n",
    "### Candidate value $\\tilde{c}^{<t>}$\n",
    "Là một tensor chứa thông tin từ current time step mà *có thể* sẽ đc lưu trong current cell state $c^{<t>}$. Phần candidate value mà đc đi qua sẽ phụ thuộc vào update gate.\n",
    "$$\n",
    "\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{a}^{\\langle t - 1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right)\n",
    "$$\n",
    "$\\mathbf{W}_c$ có shape $\\left(n_a, n_a+n_x\\right)$, $\\mathbf{b}_c$ có shape $\\left(n_a, 1\\right)$, $\\tilde{c}^{<t>}$ có shape $\\left(n_a, m\\right)$.\n",
    "    \n",
    "### Update gate $\\Gamma_i$\n",
    "Đc dùng để xđ phần nào của candidate $\\tilde{c}^{<t>}$ để cộng vào cell state $c^{<t>}$. Khi một unit trong update gate có giá trị gần $1$, nó cho phép giá trị trong $\\tilde{c}^{<t>}$ đc truyền vào $c^{<t>}$. Ngược lại khi gần $0$.\n",
    "$$\n",
    "\\mathbf{\\Gamma}_i^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_i[a^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_i)\n",
    "$$\n",
    "$\\mathbf{W}_i$ có shape $\\left(n_a, n_a+n_x\\right)$, $\\mathbf{b}_i$ có shape $\\left(n_a, 1\\right)$, $\\Gamma_i^{<t>}$ có shape $\\left(n_a, m\\right)$.\n",
    "    \n",
    "### Cell state $c^{<t>}$\n",
    "Là memory đc truyền tới các future time steps. New cell state là kết hợp của previous cell state và candidate value.\n",
    "$$\n",
    "\\mathbf{c}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_f^{\\langle t \\rangle}* \\mathbf{c}^{\\langle t-1 \\rangle} + \\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} *\\mathbf{\\tilde{c}}^{\\langle t \\rangle}\n",
    "$$\n",
    "Previous cell state $c^{<t-1>}$ đc điều chỉnh bằng forget gate $\\Gamma_f^{<t>}$, còn candidate value $\\tilde{c}^{<t>}$ đc điều chỉnh bằng update gate $\\Gamma_i^{<t>}$. \n",
    "\n",
    "### Output gate $\\Gamma_o$\n",
    "Quyết định cái nào sẽ đc gửi đi để tính prediction.\n",
    "$$\n",
    "\\mathbf{\\Gamma}_o^{\\langle t \\rangle}=  \\sigma(\\mathbf{W}_o[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})\n",
    "$$\n",
    "\n",
    "$\\mathbf{W}_o$ có shape $\\left(n_a, n_a+n_x\\right)$, $\\mathbf{b}_o$ có shape $\\left(n_a, 1\\right)$, $\\Gamma_o^{<t>}$ có shape $\\left(n_a, m\\right)$.\n",
    "    \n",
    "### Hidden state $a^{<t>}$\n",
    "Đc truyền tới các cell’s next time step và để tính ba gates trong next step, cũng như để tính prediction $\\hat{y}^{<t>}$.\n",
    "$$\n",
    "\\mathbf{a}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_o^{\\langle t \\rangle} * \\tanh(\\mathbf{c}^{\\langle t \\rangle})\n",
    "$$\n",
    "Output gate giống như một mask để giữ hoặc ko giữ giá trị của $\\tanh(\\mathbf{c}^{\\langle t \\rangle}$ trong $a^{<t>}$. \n",
    "    \n",
    "### Prediction $\\hat{y}^{<t>}$\n",
    "Trong case này prediction là classification, nên ta dùng softmax:\n",
    "$$\n",
    "\\hat{\\mathbf{y}}^{\\langle t \\rangle} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{a}^{\\langle t \\rangle} + \\mathbf{b}_{y})\n",
    "$$\n",
    "$\\mathbf{W}_y$ có shape $\\left(n_y, n_a\\right)$, $\\mathbf{b}_y$ có shape $\\left(n_y, 1\\right)$, $\\hat{y}^{<t>}$ có shape $\\left(n_y, m\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wf = parameters[\"Wf\"] # forget gate weight\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"] # update gate weight (notice the variable name)\n",
    "    bi = parameters[\"bi\"] # (notice the variable name)\n",
    "    Wc = parameters[\"Wc\"] # candidate value weight\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"] # output gate weight\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"] # prediction weight\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # Retrieve dimensions from shapes of xt and Wy\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    # Concatenate a_prev and xt (≈1 line)\n",
    "    concat = np.concatenate([a_prev,xt])\n",
    "\n",
    "    # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)\n",
    "    ft = sigmoid(np.dot(Wf,concat) + bf) # Forget Gate\n",
    "    it = sigmoid(np.dot(Wi,concat) + bi) # Update Gate\n",
    "    cct = np.tanh(np.dot(Wc,concat) + bc) # Candidate Value\n",
    "    c_next = c_prev*ft + cct*it # C_t\n",
    "    ot = sigmoid(np.dot(Wo,concat) + bo) # output gate\n",
    "    a_next = ot*(np.tanh(c_next)) #a_t\n",
    "    \n",
    "    # Compute prediction of the LSTM cell (≈1 line)\n",
    "    yt_pred = softmax(np.dot(Wy,a_next) + by)\n",
    "\n",
    "    # store values needed for backward propagation in cache\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "\n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Forward Pass for LSTM\n",
    "Lặp lại các LSTM cell để xử lí một sequence of $T_x$ inputs:\n",
    "    \n",
    "![](images/LSTM_rnn.png)\n",
    "    \n",
    "Các bước thực hiện:\n",
    "1. Khởi tạo các 3D tensors: hidden state $a$ có shape $\\left(n_a, m, T_x\\right)$, cell state $c$ có shape $\\left(n_a, m, T_x\\right)$, prediction $y$ có shape $\\left(n_y, m, T_x\\right)$.\n",
    "2. Khởi tạo 2D tensor $a^{<0>}$ có shape $\\left(n_a, m\\right)$ và gán $a^{<t>}=a^{<0>}$ (là `a_next` trong code).\n",
    "3. Khởi tạo $c^{<t>}$ là vector không, có shape $\\left(n_a, m\\right)$ (là `c_next` trong code).\n",
    "4. Tại mỗi time step $t$:\n",
    "    - Lấy $x^{<t>}$ là một 2D slice of $x$ tại time step $t$, có shape $\\left(n_x, m\\right)$\n",
    "    - Chạy lstm`_cell_forward` để cập nhật $a^{<t>}$, $c^{<t>}$ và tính $\\hat{y}^{<t>}$.\n",
    "    - Lưu $a^{<t>}$ vào $a$, $c^{<t>}$ vào $c$, và $\\hat{y}^{<t>}$ vào $\\hat{y}$ tại vị trí $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, parameters):\n",
    "    # Initialize \"caches\", which will track the list of all the caches\n",
    "    caches = []\n",
    "    \n",
    "    #Wy = parameters['Wy'] # Save parameters in local variables in case you want to use Wy instead of parameters['Wy']\n",
    "    # Retrieve dimensions from shapes of x and parameters['Wy'] (≈2 lines)\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters['Wy'].shape\n",
    "    \n",
    "    # initialize \"a\", \"c\" and \"y\" with zeros (≈3 lines)\n",
    "    a = np.zeros((n_a,m,T_x))\n",
    "    c = np.zeros((n_a,m,T_x))\n",
    "    y = np.zeros((n_y,m,T_x))\n",
    "    \n",
    "    # Initialize a_next and c_next (≈2 lines)\n",
    "    a_next = a0\n",
    "    c_next = np.zeros((n_a,m))\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Get the 2D slice 'xt' from the 3D input 'x' at time step 't'\n",
    "        xt = x[:,:,t]\n",
    "        # Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(xt, a_next, c_next, parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the next cell state (≈1 line)\n",
    "        c[:,:,t]  = c_next\n",
    "        # Save the value of the prediction in y (≈1 line)\n",
    "        y[:,:,t] = yt\n",
    "        # Append the cache into caches (≈1 line)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "\n",
    "    return a, y, c, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "* Các gates đều có giá trị thuộc $[0,1 ]$, candidate value và hidden state có giá trị thuộc $[-1, 1]$.\n",
    "* LSTM khác RNN ở chỗ là nó dùng thêm một cell state, giống như một long-term memory, để xử lí vấn đề vanishing gradients.\n",
    "* Forget gate sẽ xđ input units nào sẽ đc nhớ để truyền tiếp hoặc quên đi. Update gate sẽ xđ information nào sẽ bị bỏ đi hay thêm vào. Output gate sẽ xđ cái gì sẽ đc gửi đi để tính output."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
