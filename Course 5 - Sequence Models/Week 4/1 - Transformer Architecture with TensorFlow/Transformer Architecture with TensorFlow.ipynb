{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbzZLqIPv6b7",
    "outputId": "19f2fc2b-6f1d-4b43-fd50-4c513e3936fd"
   },
   "source": [
    "# Transformer Network\n",
    "\n",
    "Welcome to Week 4's assignment, the last assignment of Course 5 of the Deep Learning Specialization! And congratulations on making it to the last assignment of the entire Deep Learning Specialization - you're almost done!\n",
    "\n",
    "Ealier in the course, you've implemented sequential neural networks such as RNNs, GRUs, and LSTMs. In this notebook you'll explore the Transformer architecture, a neural network that takes advantage of parallel processing and allows you to substantially speed up the training process. \n",
    "\n",
    "**After this assignment you'll be able to**:\n",
    "\n",
    "* Create positional encodings to capture sequential relationships in data\n",
    "* Calculate scaled dot-product self-attention with word embeddings\n",
    "* Implement masked multi-head attention\n",
    "* Build and train a Transformer model\n",
    "\n",
    "For the last time, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Packages](#0)\n",
    "- [1 - Positional Encoding](#1)\n",
    "    - [1.1 - Sine and Cosine Angles](#1-1)\n",
    "        - [Exercise 1 - get_angles](#ex-1)\n",
    "    - [1.2 - Sine and Cosine Positional Encodings](#1-2)\n",
    "        - [Exercise 2 - positional_encoding](#ex-2)\n",
    "- [2 - Masking](#2)\n",
    "    - [2.1 - Padding Mask](#2-1)\n",
    "    - [2.2 - Look-ahead Mask](#2-2)\n",
    "- [3 - Self-Attention](#3)\n",
    "    - [Exercise 3 - scaled_dot_product_attention](#ex-3)\n",
    "- [4 - Encoder](#4)\n",
    "    - [4.1 Encoder Layer](#4-1)\n",
    "        - [Exercise 4 - EncoderLayer](#ex-4)\n",
    "    - [4.2 - Full Encoder](#4-2)\n",
    "        - [Exercise 5 - Encoder](#ex-5)\n",
    "- [5 - Decoder](#5)\n",
    "    - [5.1 - Decoder Layer](#5-1)\n",
    "        - [Exercise 6 - DecoderLayer](#ex-6)\n",
    "    - [5.2 - Full Decoder](#5-2)\n",
    "        - [Exercise 7 - Decoder](#ex-7)\n",
    "- [6 - Transformer](#6)\n",
    "    - [Exercise 8 - Transformer](#ex-8)\n",
    "- [7 - References](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Packages\n",
    "\n",
    "Run the following cell to load the packages you'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_OpwqWL2QH5G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 03:14:22.170799: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-15 03:14:22.276675: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-15 03:14:22.276690: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-15 03:14:22.795141: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-15 03:14:22.795179: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-15 03:14:22.795185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization\n",
    "from transformers import DistilBertTokenizerFast #, TFDistilBertModel\n",
    "from transformers import TFDistilBertForTokenClassification\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Positional Encoding\n",
    "\n",
    "In sequence to sequence tasks, the relative order of your data is extremely important to its meaning. When you were training sequential neural networks such as RNNs, you fed your inputs into the network in order. Information about the order of your data was automatically fed into your model.  However, when you train a Transformer network, you feed your data into the model all at once. While this dramatically reduces training time, there is no information about the order of your data. This is where positional encoding is useful - you can specifically encode the positions of your inputs and pass them into the network using these sine and cosine formulas:\n",
    "    \n",
    "$$\n",
    "PE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{1}$$\n",
    "<br>\n",
    "$$\n",
    "PE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "\\tag{2}$$\n",
    "\n",
    "* $d$ is the dimension of the word embedding and positional encoding\n",
    "* $pos$ is the position of the word.\n",
    "* $i$ refers to each of the different dimensions of the positional encoding.\n",
    "\n",
    "The values of the sine and cosine equations are small enough (between -1 and 1) that when you add the positional encoding to a word embedding, the word embedding is not significantly distorted. The sum of the positional encoding and word embeding is ultimately what is fed into the model. Using a combination of these two equations helps your Transformer network attend to the relative positions of your input data. Note that while in the lectures Andrew uses vertical vectors but in this assignment, all vectors are horizontal. All matrix multiplications should be adjusted accordingly.\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - Sine and Cosine Angles\n",
    "\n",
    "Get the possible angles used to compute the positional encodings by calculating the inner term of the sine and cosine equations: \n",
    "\n",
    "$$\\frac{pos}{10000^{\\frac{2i}{d}}} \\tag{3}$$\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - get_angles\n",
    "\n",
    "Implement the function `get_angles()` to calculate the possible angles for the sine and cosine  positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bPzwMVfcQpT-"
   },
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION get_angles\n",
    "def get_angles(pos, i, d):\n",
    "    \"\"\"\n",
    "    Get the angles for the positional encoding\n",
    "    \n",
    "    Arguments:\n",
    "        pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
    "        i --   Row vector containing the dimension span [[0, 1, 2, ..., M-1]]\n",
    "        d(integer) -- Encoding size\n",
    "    \n",
    "    Returns:\n",
    "        angles -- (pos, d) numpy array \n",
    "    \"\"\"\n",
    "    # START CODE HERE\n",
    "    angles = pos/ (np.power(10000, (2 * (i//2)) / np.float32(d)))\n",
    "    # END CODE HERE\n",
    "    \n",
    "    return angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00],\n",
       "       [1.e+00, 1.e+00, 1.e-01, 1.e-01, 1.e-02, 1.e-02, 1.e-03, 1.e-03],\n",
       "       [2.e+00, 2.e+00, 2.e-01, 2.e-01, 2.e-02, 2.e-02, 2.e-03, 2.e-03],\n",
       "       [3.e+00, 3.e+00, 3.e-01, 3.e-01, 3.e-02, 3.e-02, 3.e-03, 3.e-03]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "def get_angles_test(target):\n",
    "    position = 4\n",
    "    d_model = 16\n",
    "    pos_m = np.arange(position)[:, np.newaxis]\n",
    "    dims = np.arange(d_model)[np.newaxis, :]\n",
    "\n",
    "    result = target(pos_m, dims, d_model)\n",
    "\n",
    "    assert type(result) == np.ndarray, \"You must return a numpy ndarray\"\n",
    "    assert result.shape == (position, d_model), f\"Wrong shape. We expected: ({position}, {d_model})\"\n",
    "    assert np.sum(result[0, :]) == 0\n",
    "    assert np.isclose(np.sum(result[:, 0]), position * (position - 1) / 2)\n",
    "    even_cols =  result[:, 0::2]\n",
    "    odd_cols = result[:,  1::2]\n",
    "    assert np.all(even_cols == odd_cols), \"Submatrices of odd and even columns must be equal\"\n",
    "    limit = (position - 1) / np.power(10000,14.0/16.0)\n",
    "    assert np.isclose(result[position - 1, d_model -1], limit ), f\"Last value must be {limit}\"\n",
    "\n",
    "    print(\"\\033[92mAll tests passed\")\n",
    "\n",
    "get_angles_test(get_angles)\n",
    "\n",
    "# Example\n",
    "position = 4\n",
    "d_model = 8\n",
    "pos_m = np.arange(position)[:, np.newaxis]\n",
    "dims = np.arange(d_model)[np.newaxis, :]\n",
    "get_angles(pos_m, dims, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - Sine and Cosine Positional Encodings\n",
    "\n",
    "Now you can use the angles you computed to calculate the sine and cosine positional encodings.\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "PE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - positional_encoding\n",
    "\n",
    "Implement the function `positional_encoding()` to calculate the sine and cosine  positional encodings\n",
    "\n",
    "**Reminder:** Use the sine equation when $i$ is an even number and the cosine equation when $i$ is an odd number.\n",
    "\n",
    "#### Additional Hints\n",
    "* You may find \n",
    "[np.newaxis](https://numpy.org/doc/stable/reference/arrays.indexing.html) useful depending on the implementation you choose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "y78txxoHQtwG"
   },
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION positional_encoding\n",
    "def positional_encoding(positions, d):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        positions (int) -- Maximum number of positions to be encoded \n",
    "        d (int) -- Encoding size \n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding -- (1, position, d_model) A matrix with the positional encodings\n",
    "    \"\"\"\n",
    "    # START CODE HERE\n",
    "    # initialize a matrix angle_rads of all the angles \n",
    "    angle_rads = get_angles(np.arange(positions)[:, np.newaxis],\n",
    "                            np.arange(d)[ np.newaxis,:],\n",
    "                            d)\n",
    "  \n",
    "    # -> angle_rads has dim (positions,d)\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    # END CODE HERE\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "jYiWrawRQvuv",
    "outputId": "cfccc7c9-428e-4b08-d969-e3090fafc1ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 03:14:24.748249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-15 03:14:24.748604: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-15 03:14:24.748641: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-15 03:14:24.748671: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-01-15 03:14:24.748699: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-01-15 03:14:24.748726: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-15 03:14:24.748753: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-15 03:14:24.748780: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-01-15 03:14:24.748786: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-01-15 03:14:24.749261: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "def positional_encoding_test(target):\n",
    "    position = 8\n",
    "    d_model = 16\n",
    "\n",
    "    pos_encoding = target(position, d_model)\n",
    "    sin_part = pos_encoding[:, :, 0::2]\n",
    "    cos_part = pos_encoding[:, :, 1::2]\n",
    "\n",
    "    assert tf.is_tensor(pos_encoding), \"Output is not a tensor\"\n",
    "    assert pos_encoding.shape == (1, position, d_model), f\"Wrong shape. We expected: (1, {position}, {d_model})\"\n",
    "\n",
    "    ones = sin_part ** 2  +  cos_part ** 2\n",
    "    assert np.allclose(ones, np.ones((1, position, d_model // 2))), \"Sum of square pairs must be 1 = sin(a)**2 + cos(a)**2\"\n",
    "    \n",
    "    angs = np.arctan(sin_part / cos_part)\n",
    "    angs[angs < 0] += np.pi\n",
    "    angs[sin_part.numpy() < 0] += np.pi\n",
    "    angs = angs % (2 * np.pi)\n",
    "    \n",
    "    pos_m = np.arange(position)[:, np.newaxis]\n",
    "    dims = np.arange(d_model)[np.newaxis, :]\n",
    "\n",
    "    trueAngs = get_angles(pos_m, dims, d_model)[:, 0::2] % (2 * np.pi)\n",
    "    \n",
    "    assert np.allclose(angs[0], trueAngs), \"Did you apply sin and cos to even and odd parts respectively?\"\n",
    " \n",
    "    print(\"\\033[92mAll tests passed\")\n",
    "\n",
    "    \n",
    "positional_encoding_test(positional_encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice work calculating the positional encodings! Now you can visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('d')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents a positional encoding - notice how none of the rows are identical! You have created a unique positional encoding for each of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Masking\n",
    "\n",
    "There are two types of masks that are useful when building your Transformer network: the *padding mask* and the *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in your input sentence. \n",
    "\n",
    "<a name='2-1'></a>\n",
    "### 2.1 - Padding Mask\n",
    "\n",
    "Oftentimes your input sequence will exceed the maximum length of a sequence your network can process. Let's say the maximum length of your model is five, it is fed the following sequences:\n",
    "\n",
    "    [[\"Do\", \"you\", \"know\", \"when\", \"Jane\", \"is\", \"going\", \"to\", \"visit\", \"Africa\"], \n",
    "     [\"Jane\", \"visits\", \"Africa\", \"in\", \"September\" ],\n",
    "     [\"Exciting\", \"!\"]\n",
    "    ]\n",
    "\n",
    "which might get vectorized as:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600]\n",
    "    ]\n",
    "    \n",
    "When passing sequences into a transformer model, it is important that they are of uniform length. You can achieve this by padding the sequence with zeros, and truncating sentences that exceed the maximum length of your model:\n",
    "\n",
    "    [[ 71, 121, 4, 56, 99],\n",
    "     [ 2344, 345, 1284, 15, 0],\n",
    "     [ 56, 1285, 15, 181, 545],\n",
    "     [ 87, 600, 0, 0, 0],\n",
    "    ]\n",
    "    \n",
    "Sequences longer than the maximum length of five will be truncated, and zeros will be added to the truncated sequence to achieve uniform length. Similarly, for sequences shorter than the maximum length, they zeros will also be added for padding. However, these zeros will affect the softmax calculation - this is when a padding mask comes in handy! By multiplying a padding mask by -1e9 and adding it to your sequence, you mask out the zeros by setting them to close to negative infinity. We'll implement this for you so you can get to the fun of building the Transformer network! 😇 Just make sure you go through the code so you can correctly implement padding when building your model. \n",
    "\n",
    "After masking, your input should go from `[87, 600, 0, 0, 0]` to `[87, 600, -1e9, -1e9, -1e9]`, so that when you take the softmax, the zeros don't affect the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JOL9XWsFQxxo"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    Creates a matrix mask for the padding cells\n",
    "    \n",
    "    Arguments:\n",
    "        seq -- (n, m) matrix\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (n, 1, 1, m) binary tensor\n",
    "    \"\"\"\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5J5FFjklQ1Fz",
    "outputId": "8319446f-3ed4-406a-cf38-ca2b08142ff4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 1. 1. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 0. 0.]]]], shape=(3, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[7., 6., 0., 0., 1.], [1., 2., 3., 0., 0.], [0., 0., 0., 4., 5.]])\n",
    "print(create_padding_mask(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we multiply this mask by -1e9 and add it to the sample input sequences, the zeros are essentially set to negative infinity. Notice the difference when taking the softmax of the original sequence and the masked sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[7.2876644e-01 2.6809821e-01 6.6454901e-04 6.6454901e-04 1.8064314e-03]\n",
      " [8.4437378e-02 2.2952460e-01 6.2391251e-01 3.1062774e-02 3.1062774e-02]\n",
      " [4.8541026e-03 4.8541026e-03 4.8541026e-03 2.6502505e-01 7.2041273e-01]], shape=(3, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[7.2973627e-01 2.6845497e-01 0.0000000e+00 0.0000000e+00\n",
      "    1.8088354e-03]\n",
      "   [2.4472848e-01 6.6524094e-01 0.0000000e+00 0.0000000e+00\n",
      "    9.0030573e-02]\n",
      "   [6.6483542e-03 6.6483542e-03 0.0000000e+00 0.0000000e+00\n",
      "    9.8670328e-01]]]\n",
      "\n",
      "\n",
      " [[[7.3057157e-01 2.6876226e-01 6.6619506e-04 0.0000000e+00\n",
      "    0.0000000e+00]\n",
      "   [9.0030573e-02 2.4472848e-01 6.6524094e-01 0.0000000e+00\n",
      "    0.0000000e+00]\n",
      "   [3.3333334e-01 3.3333334e-01 3.3333334e-01 0.0000000e+00\n",
      "    0.0000000e+00]]]\n",
      "\n",
      "\n",
      " [[[0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6894143e-01\n",
      "    7.3105860e-01]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 4.9999997e-01\n",
      "    4.9999997e-01]\n",
      "   [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6894143e-01\n",
      "    7.3105860e-01]]]], shape=(3, 1, 3, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.activations.softmax(x))\n",
    "print(tf.keras.activations.softmax(x + create_padding_mask(x) * -1.0e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - Look-ahead Mask\n",
    "\n",
    "The look-ahead mask follows similar intuition. In training, you will have access to the complete correct output of your training example. The look-ahead mask helps your model pretend that it correctly predicted a part of the output and see if, *without looking ahead*, it can correctly predict the next output. \n",
    "\n",
    "For example, if the expected correct output is `[1, 2, 3]` and you wanted to see if given that the model correctly predicted the first value it could predict the second value, you would mask out the second and third values. So you would input the masked sequence `[1, -1e9, -1e9]` and see if it could generate `[1, 2, -1e9]`.\n",
    "\n",
    "Just because you've worked so hard, we'll also implement this mask for you 😇😇. Again, take a close look at the code so you can effictively implement it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9O9UbM31Q3hK"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"\n",
    "    Returns an upper triangular matrix filled with ones\n",
    "    \n",
    "    Arguments:\n",
    "        size -- matrix size\n",
    "    \n",
    "    Returns:\n",
    "        mask -- (size, size) tensor\n",
    "    \"\"\"\n",
    "    mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfzHoVj9Q5nG",
    "outputId": "300e76ec-77d0-460a-b6df-71e40de86606",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG0gPyv0oDBi"
   },
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Self-Attention\n",
    "\n",
    "As the authors of the Transformers paper state, \"Attention is All You Need\". \n",
    "\n",
    "<img src=\"self-attention.png\" alt=\"Encoder\" width=\"600\"/>\n",
    "<caption><center><font color='purple'><b>Figure 1: Self-Attention calculation visualization</font></center></caption>\n",
    "    \n",
    "The use of self-attention paired with traditional convolutional networks allows for the parallization which speeds up training. You will implement **scaled dot product attention** which takes in a query, key, value, and a mask as inputs to returns rich, attention-based vector representations of the words in your sequence. This type of self-attention can be mathematically expressed as:\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{4}\\\n",
    "$$\n",
    "\n",
    "* $Q$ is the matrix of queries \n",
    "* $K$ is the matrix of keys\n",
    "* $V$ is the matrix of values\n",
    "* $M$ is the optional mask you choose to apply \n",
    "* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - scaled_dot_product_attention \n",
    "\n",
    "    Implement the function `scaled_dot_product_attention()` to create attention-based representations\n",
    "**Reminder**: The boolean mask parameter can be passed in as `none` or as either padding or look-ahead. Multiply it by -1e9 before applying the softmax. \n",
    "\n",
    "**Additional Hints**\n",
    "* You may find [tf.matmul](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) useful for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CSysk_rjQ7lp"
   },
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION scaled_dot_product_attention\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type(padding or look ahead) \n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "    Arguments:\n",
    "        q -- query shape == (..., seq_len_q, depth)\n",
    "        k -- key shape == (..., seq_len_k, depth)\n",
    "        v -- value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        output -- attention_weights\n",
    "    \"\"\"\n",
    "    # START CODE HERE\n",
    "    \n",
    "    # Q*K'\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    # attention_weights * V\n",
    "    output = tf.matmul(attention_weights, v)   # (..., seq_len_q, depth_v)\n",
    "    \n",
    "    # END CODE HERE\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "def scaled_dot_product_attention_test(target):\n",
    "    q = np.array([[1, 0, 1, 1], [0, 1, 1, 1], [1, 0, 0, 1]]).astype(np.float32)\n",
    "    k = np.array([[1, 1, 0, 1], [1, 0, 1, 1 ], [0, 1, 1, 0], [0, 0, 0, 1]]).astype(np.float32)\n",
    "    v = np.array([[0, 0], [1, 0], [1, 0], [1, 1]]).astype(np.float32)\n",
    "\n",
    "    attention, weights = target(q, k, v, None)\n",
    "    assert tf.is_tensor(weights), \"Weights must be a tensor\"\n",
    "    assert tuple(tf.shape(weights).numpy()) == (q.shape[0], k.shape[1]), f\"Wrong shape. We expected ({q.shape[0]}, {k.shape[1]})\"\n",
    "    assert np.allclose(weights, [[0.2589478,  0.42693272, 0.15705977, 0.15705977],\n",
    "                                   [0.2772748,  0.2772748,  0.2772748,  0.16817567],\n",
    "                                   [0.33620113, 0.33620113, 0.12368149, 0.2039163 ]])\n",
    "\n",
    "    assert tf.is_tensor(attention), \"Output must be a tensor\"\n",
    "    assert tuple(tf.shape(attention).numpy()) == (q.shape[0], v.shape[1]), f\"Wrong shape. We expected ({q.shape[0]}, {v.shape[1]})\"\n",
    "    assert np.allclose(attention, [[0.74105227, 0.15705977],\n",
    "                                   [0.7227253,  0.16817567],\n",
    "                                   [0.6637989,  0.2039163 ]])\n",
    "\n",
    "    mask = np.array([[0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 0]])\n",
    "    attention, weights = target(q, k, v, mask)\n",
    "\n",
    "    assert np.allclose(weights, [[0.30719590187072754, 0.5064803957939148, 0.0, 0.18632373213768005],\n",
    "                                 [0.3836517333984375, 0.3836517333984375, 0.0, 0.2326965481042862],\n",
    "                                 [0.3836517333984375, 0.3836517333984375, 0.0, 0.2326965481042862]]), \"Wrong masked weights\"\n",
    "    assert np.allclose(attention, [[0.6928040981292725, 0.18632373213768005],\n",
    "                                   [0.6163482666015625, 0.2326965481042862], \n",
    "                                   [0.6163482666015625, 0.2326965481042862]]), \"Wrong masked attention\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed\")\n",
    "    \n",
    "scaled_dot_product_attention_test(scaled_dot_product_attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! You can now implement self-attention. With that, you can start building the encoder block! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blS0pEpTqRVI"
   },
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Encoder\n",
    "\n",
    "The Transformer Encoder layer pairs self-attention and convolutional neural network style of processing to improve the speed of training and passes K and V matrices to the Decoder, which you'll build later in the assignment. In this section of the assignment, you will implement the Encoder by pairing multi-head attention and a feed forward neural network (Figure 2a). \n",
    "<img src=\"encoder_layer.png\" alt=\"Encoder\" width=\"250\"/>\n",
    "<caption><center><font color='purple'><b>Figure 2a: Transformer encoder layer</font></center></caption>\n",
    "\n",
    "* `MultiHeadAttention` you can think of as computing the self-attention several times to detect different features. \n",
    "* Feed forward neural network contains two Dense layers which we'll implement as the function `FullyConnected`\n",
    "\n",
    "Your input sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a *feed forward neural network*. The exact same feed forward network is independently applied to each position.\n",
    "   \n",
    "* For the `MultiHeadAttention` layer, you will use the [Keras implementation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention). If you're curious about how to split the query matrix Q, key matrix K, and value matrix V into different heads, you can look through the implementation. \n",
    "* You will also use the [Sequential API](https://keras.io/api/models/sequential/) with two dense layers to built the feed forward neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sC5vJhz29vZR"
   },
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R65WbX5wqYYH"
   },
   "source": [
    "<a name='4-1'></a>\n",
    "### 4.1 Encoder Layer\n",
    "\n",
    "Now you can pair multi-head attention and feed forward neural network together in an encoder layer! You will also use residual connections and layer normalization to help speed up training (Figure 2a).\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - EncoderLayer\n",
    "\n",
    "Implement `EncoderLayer()` using the `call()` method\n",
    "\n",
    "In this exercise, you will implement one encoder block (Figure 2) using the `call()` method. The function should perform the following steps: \n",
    "1. You will pass the Q, V, K matrices and a boolean mask to a multi-head attention layer. Remember that to compute *self*-attention Q, V and K should be the same.\n",
    "2. Next, you will pass the output of the multi-head attention layer to a dropout layer. Don't forget to use the `training` parameter to set the mode of your model. \n",
    "3. Now add a skip connection by adding your original input `x` and the output of the dropout layer. \n",
    "4. After adding the skip connection, pass the output through the first layer normalization.\n",
    "5. Finally, repeat steps 1-4 but with the feed forward neural network instead of the multi-head attention layer.\n",
    "\n",
    "**Additional Hints**:\n",
    "* The `__init__` method creates all the layers that will be accesed by the the `call` method. Wherever you want to use a layer defined inside  the `__init__`  method you will have to use the syntax `self.[insert layer name]`. \n",
    "* You will find the documentation of [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) helpful. *Note that if query, key and value are the same, then this function performs self-attention.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tIufbrc-9_2u"
   },
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION EncoderLayer\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network. \n",
    "    This archirecture includes a residual connection around each of the two \n",
    "    sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim)\n",
    "\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
    "                                  fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        # START CODE HERE\n",
    "        # calculate self-attention using mha(~1 line)\n",
    "        #-> To compute self-attention Q, V and K should be the same (x)\n",
    "        self_attn_output = self.mha(x, x, x, mask) # Self attention (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        # apply dropout layer to the self-attention output (~1 line)\n",
    "        self_attn_output = self.dropout1(self_attn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization on sum of the input and the attention output to get the  \n",
    "        # output of the multi-head attention layer (~1 line)\n",
    "        mult_attn_out = self.layernorm1(x + self_attn_output)  # (batch_size, input_seq_len, embedding_dim)\n",
    "\n",
    "        # pass the output of the multi-head attention layer through a ffn (~1 line)\n",
    "        ffn_output = self.ffn(mult_attn_out)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        # apply dropout layer to ffn output (~1 line)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization on sum of the output from multi-head attention and ffn output to get the\n",
    "        # output of the encoder layer (~1 line)\n",
    "        encoder_layer_out = self.layernorm2(ffn_output + mult_attn_out)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        # END CODE HERE\n",
    "        \n",
    "        return encoder_layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.3621659  -1.240122   -0.6083576   0.4863137 ]\n",
      "  [-1.6851962   0.24027383  0.5514655   0.8934569 ]\n",
      "  [ 1.1823493  -1.2326484  -0.71016055  0.76045966]]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Wrong values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/hieunm44aic/AI/dl-specialization/Course 5 - Sequence Models/Week 4/1 - Transformer Architecture with TensorFlow/Transformer Architecture with TensorFlow.ipynb Cell 30'\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hieunm44aic/AI/dl-specialization/Course%205%20-%20Sequence%20Models/Week%204/1%20-%20Transformer%20Architecture%20with%20TensorFlow/Transformer%20Architecture%20with%20TensorFlow.ipynb#ch0000029?line=11'>12</a>\u001b[0m     \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mallclose(encoded\u001b[39m.\u001b[39mnumpy(), \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hieunm44aic/AI/dl-specialization/Course%205%20-%20Sequence%20Models/Week%204/1%20-%20Transformer%20Architecture%20with%20TensorFlow/Transformer%20Architecture%20with%20TensorFlow.ipynb#ch0000029?line=12'>13</a>\u001b[0m                        [[\u001b[39m-\u001b[39m\u001b[39m0.5214877\u001b[39m , \u001b[39m-\u001b[39m\u001b[39m1.001476\u001b[39m  , \u001b[39m-\u001b[39m\u001b[39m0.12321664\u001b[39m,  \u001b[39m1.6461804\u001b[39m ],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hieunm44aic/AI/dl-specialization/Course%205%20-%20Sequence%20Models/Week%204/1%20-%20Transformer%20Architecture%20with%20TensorFlow/Transformer%20Architecture%20with%20TensorFlow.ipynb#ch0000029?line=13'>14</a>\u001b[0m                        [\u001b[39m-\u001b[39m\u001b[39m1.3114998\u001b[39m ,  \u001b[39m1.2167752\u001b[39m , \u001b[39m-\u001b[39m\u001b[39m0.5830886\u001b[39m ,  \u001b[39m0.6778133\u001b[39m ],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hieunm44aic/AI/dl-specialization/Course%205%20-%20Sequence%20Models/Week%204/1%20-%20Transformer%20Architecture%20with%20TensorFlow/Transformer%20Architecture%20with%20TensorFlow.ipynb#ch0000029?line=14'>15</a>\u001b[0m                        [ \u001b[39m0.25485858\u001b[39m,  \u001b[39m0.3776546\u001b[39m , \u001b[39m-\u001b[39m\u001b[39m1.6564771\u001b[39m ,  \u001b[39m1.023964\u001b[39m  ]],), \u001b[39m\"\u001b[39m\u001b[39mWrong values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hieunm44aic/AI/dl-specialization/Course%205%20-%20Sequence%20Models/Week%204/1%20-%20Transformer%20Architecture%20with%20TensorFlow/Transformer%20Architecture%20with%20TensorFlow.ipynb#ch0000029?line=16'>17</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[92mAll tests passed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hieunm44aic/AI/dl-specialization/Course%205%20-%20Sequence%20Models/Week%204/1%20-%20Transformer%20Architecture%20with%20TensorFlow/Transformer%20Architecture%20with%20TensorFlow.ipynb#ch0000029?line=19'>20</a>\u001b[0m EncoderLayer_test(EncoderLayer)\n",
      "\u001b[1;32m/home/hieunm44aic/AI/dl-specialization/Course 5 - Sequence Models/Week 4/1 - Transformer Architecture with TensorFlow/Transformer Architecture with TensorFlow.ipynb Cell 30'\u001b[0m in \u001b[0;36mEncoderLayer_test\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hieunm44aic/AI/dl-specialization/Course%205%20-%20Sequence%20Models/Week%204/1%20-%20Transformer%20Architecture%20with%20TensorFlow/Transformer%20Architecture%20with%20TensorFlow.ipynb#ch0000029?line=8'>9</a>\u001b[0m \u001b[39massert\u001b[39;00m tf\u001b[39m.\u001b[39mis_tensor(encoded), \u001b[39m\"\u001b[39m\u001b[39mWrong type. Output must be a tensor\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hieunm44aic/AI/dl-specialization/Course%205%20-%20Sequence%20Models/Week%204/1%20-%20Transformer%20Architecture%20with%20TensorFlow/Transformer%20Architecture%20with%20TensorFlow.ipynb#ch0000029?line=9'>10</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtuple\u001b[39m(tf\u001b[39m.\u001b[39mshape(encoded)\u001b[39m.\u001b[39mnumpy()) \u001b[39m==\u001b[39m (\u001b[39m1\u001b[39m, q\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], q\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrong shape. We expected ((1, \u001b[39m\u001b[39m{\u001b[39;00mq\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mq\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m))\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hieunm44aic/AI/dl-specialization/Course%205%20-%20Sequence%20Models/Week%204/1%20-%20Transformer%20Architecture%20with%20TensorFlow/Transformer%20Architecture%20with%20TensorFlow.ipynb#ch0000029?line=11'>12</a>\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mallclose(encoded\u001b[39m.\u001b[39mnumpy(), \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hieunm44aic/AI/dl-specialization/Course%205%20-%20Sequence%20Models/Week%204/1%20-%20Transformer%20Architecture%20with%20TensorFlow/Transformer%20Architecture%20with%20TensorFlow.ipynb#ch0000029?line=12'>13</a>\u001b[0m                    [[\u001b[39m-\u001b[39m\u001b[39m0.5214877\u001b[39m , \u001b[39m-\u001b[39m\u001b[39m1.001476\u001b[39m  , \u001b[39m-\u001b[39m\u001b[39m0.12321664\u001b[39m,  \u001b[39m1.6461804\u001b[39m ],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hieunm44aic/AI/dl-specialization/Course%205%20-%20Sequence%20Models/Week%204/1%20-%20Transformer%20Architecture%20with%20TensorFlow/Transformer%20Architecture%20with%20TensorFlow.ipynb#ch0000029?line=13'>14</a>\u001b[0m                    [\u001b[39m-\u001b[39m\u001b[39m1.3114998\u001b[39m ,  \u001b[39m1.2167752\u001b[39m , \u001b[39m-\u001b[39m\u001b[39m0.5830886\u001b[39m ,  \u001b[39m0.6778133\u001b[39m ],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hieunm44aic/AI/dl-specialization/Course%205%20-%20Sequence%20Models/Week%204/1%20-%20Transformer%20Architecture%20with%20TensorFlow/Transformer%20Architecture%20with%20TensorFlow.ipynb#ch0000029?line=14'>15</a>\u001b[0m                    [ \u001b[39m0.25485858\u001b[39m,  \u001b[39m0.3776546\u001b[39m , \u001b[39m-\u001b[39m\u001b[39m1.6564771\u001b[39m ,  \u001b[39m1.023964\u001b[39m  ]],), \u001b[39m\"\u001b[39m\u001b[39mWrong values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hieunm44aic/AI/dl-specialization/Course%205%20-%20Sequence%20Models/Week%204/1%20-%20Transformer%20Architecture%20with%20TensorFlow/Transformer%20Architecture%20with%20TensorFlow.ipynb#ch0000029?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[92mAll tests passed\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Wrong values"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "def EncoderLayer_test(target):\n",
    "    q = np.array([[[1, 0, 1, 1], [0, 1, 1, 1], [1, 0, 0, 1]]]).astype(np.float32)\n",
    "    encoder_layer1 = EncoderLayer(4, 2, 8)\n",
    "    tf.random.set_seed(10)\n",
    "    encoded = encoder_layer1(q, True, np.array([[1, 0, 1]]))\n",
    "    \n",
    "    assert tf.is_tensor(encoded), \"Wrong type. Output must be a tensor\"\n",
    "    assert tuple(tf.shape(encoded).numpy()) == (1, q.shape[1], q.shape[2]), f\"Wrong shape. We expected ((1, {q.shape[1]}, {q.shape[2]}))\"\n",
    "\n",
    "    assert np.allclose(encoded.numpy(), \n",
    "                       [[-0.5214877 , -1.001476  , -0.12321664,  1.6461804 ],\n",
    "                       [-1.3114998 ,  1.2167752 , -0.5830886 ,  0.6778133 ],\n",
    "                       [ 0.25485858,  0.3776546 , -1.6564771 ,  1.023964  ]],), \"Wrong values\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed\")\n",
    "    \n",
    "\n",
    "EncoderLayer_test(EncoderLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Full Encoder\n",
    "\n",
    "Awesome job! You have now successfully implemented positional encoding, self-attention, and an encoder layer - give yourself a pat on the back. Now you're ready to build the full Transformer Encoder (Figure 2b), where you will embedd your input and add the positional encodings you calculated. You will then feed your encoded embeddings to a stack of Encoder layers. \n",
    "\n",
    "<img src=\"encoder.png\" alt=\"Encoder\" width=\"330\"/>\n",
    "<caption><center><font color='purple'><b>Figure 2b: Transformer Encoder</font></center></caption>\n",
    "\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - Encoder\n",
    "\n",
    "Complete the `Encoder()` function using the `call()` method to embed your input, add positional encoding, and implement multiple encoder layers \n",
    "\n",
    "In this exercise, you will initialize your Encoder with an Embedding layer, positional encoding, and multiple EncoderLayers. Your `call()` method will perform the following steps: \n",
    "1. Pass your input through the Embedding layer.\n",
    "2. Scale your embedding by multiplying it by the square root of your embedding dimension. Remember to cast the embedding dimension to data type `tf.float32` before computing the square root.\n",
    "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to your embedding.\n",
    "4. Pass the encoded embedding through a dropout layer, remembering to use the `training` parameter to set the model training mode. \n",
    "5. Pass the output of the dropout layer through the stack of encoding layers using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "7j2Tjr0K0t0I"
   },
   "outputs": [],
   "source": [
    " # UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "        \n",
    "    \"\"\"   \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.embedding_dim)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, input_seq_len)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # START CODE HERE\n",
    "        # Pass input through the Embedding layer\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
    "        # Scale embedding by multiplying it by the square root of the embedding dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim,tf.float32))\n",
    "        # Add the position encoding to embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        # Pass the encoded embedding through a dropout layer\n",
    "        x = self.dropout(x, training=training)\n",
    "        # Pass the output through the stack of encoding layers \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x,training, mask)\n",
    "        # END CODE HERE\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Wrong values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(encoderq_output\u001b[38;5;241m.\u001b[39mnumpy(), \n\u001b[1;32m     21\u001b[0m                        [[[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.40172306\u001b[39m,  \u001b[38;5;241m0.11519244\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.2322885\u001b[39m,   \u001b[38;5;241m1.5188192\u001b[39m ],\n\u001b[1;32m     22\u001b[0m                          [ \u001b[38;5;241m0.4017268\u001b[39m,   \u001b[38;5;241m0.33922842\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.6836855\u001b[39m,   \u001b[38;5;241m0.9427304\u001b[39m ],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m                          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.08761203\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1680029\u001b[39m,  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.2742313\u001b[39m,   \u001b[38;5;241m1.5298463\u001b[39m ],\n\u001b[1;32m     26\u001b[0m                          [ \u001b[38;5;241m0.2627198\u001b[39m,  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.6140151\u001b[39m,   \u001b[38;5;241m0.2212624\u001b[39m ,  \u001b[38;5;241m1.130033\u001b[39m  ]]]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[92mAll tests passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mEncoder_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEncoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36mEncoder_test\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mis_tensor(encoderq_output), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong type. Output must be a tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tf\u001b[38;5;241m.\u001b[39mshape(encoderq_output)\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;241m==\u001b[39m (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], embedding_dim), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong shape. We expected (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(encoderq_output\u001b[38;5;241m.\u001b[39mnumpy(), \n\u001b[1;32m     21\u001b[0m                    [[[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.40172306\u001b[39m,  \u001b[38;5;241m0.11519244\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.2322885\u001b[39m,   \u001b[38;5;241m1.5188192\u001b[39m ],\n\u001b[1;32m     22\u001b[0m                      [ \u001b[38;5;241m0.4017268\u001b[39m,   \u001b[38;5;241m0.33922842\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.6836855\u001b[39m,   \u001b[38;5;241m0.9427304\u001b[39m ],\n\u001b[1;32m     23\u001b[0m                      [ \u001b[38;5;241m0.4685002\u001b[39m,  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.6252842\u001b[39m,   \u001b[38;5;241m0.09368491\u001b[39m,  \u001b[38;5;241m1.063099\u001b[39m  ]],\n\u001b[1;32m     24\u001b[0m                     [[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.3489219\u001b[39m,   \u001b[38;5;241m0.31335592\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.3568854\u001b[39m,   \u001b[38;5;241m1.3924513\u001b[39m ],\n\u001b[1;32m     25\u001b[0m                      [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.08761203\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1680029\u001b[39m,  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.2742313\u001b[39m,   \u001b[38;5;241m1.5298463\u001b[39m ],\n\u001b[1;32m     26\u001b[0m                      [ \u001b[38;5;241m0.2627198\u001b[39m,  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.6140151\u001b[39m,   \u001b[38;5;241m0.2212624\u001b[39m ,  \u001b[38;5;241m1.130033\u001b[39m  ]]]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[92mAll tests passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Wrong values"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "def Encoder_test(target):\n",
    "    tf.random.set_seed(10)\n",
    "    \n",
    "    embedding_dim=4\n",
    "    \n",
    "    encoderq = target(num_layers=2,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      num_heads=2,\n",
    "                      fully_connected_dim=8,\n",
    "                      input_vocab_size=32,\n",
    "                      maximum_position_encoding=5)\n",
    "    \n",
    "    x = np.array([[2, 1, 3], [1, 2, 0]])\n",
    "    \n",
    "    encoderq_output = encoderq(x, True, None)\n",
    "    \n",
    "    assert tf.is_tensor(encoderq_output), \"Wrong type. Output must be a tensor\"\n",
    "    assert tuple(tf.shape(encoderq_output).numpy()) == (x.shape[0], x.shape[1], embedding_dim), f\"Wrong shape. We expected ({eshape[0]}, {eshape[1]}, {embedding_dim})\"\n",
    "    assert np.allclose(encoderq_output.numpy(), \n",
    "                       [[[-0.40172306,  0.11519244, -1.2322885,   1.5188192 ],\n",
    "                         [ 0.4017268,   0.33922842, -1.6836855,   0.9427304 ],\n",
    "                         [ 0.4685002,  -1.6252842,   0.09368491,  1.063099  ]],\n",
    "                        [[-0.3489219,   0.31335592, -1.3568854,   1.3924513 ],\n",
    "                         [-0.08761203, -0.1680029,  -1.2742313,   1.5298463 ],\n",
    "                         [ 0.2627198,  -1.6140151,   0.2212624 ,  1.130033  ]]]), \"Wrong values\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed\")\n",
    "    \n",
    "Encoder_test(Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Decoder\n",
    "\n",
    "The Decoder layer takes the K and V matrices generated by the Encoder and in computes the second multi-head attention layer with the Q matrix from the output (Figure 3a).\n",
    "\n",
    "<img src=\"decoder_layer.png\" alt=\"Encoder\" width=\"250\"/>\n",
    "<caption><center><font color='purple'><b>Figure 3a: Transformer Decoder layer</font></center></caption>\n",
    "\n",
    "<a name='5-1'></a>    \n",
    "### 5.1 - Decoder Layer\n",
    "Again, you'll pair multi-head attention with a feed forward neural network, but this time you'll implement two multi-head attention layers. You will also use residual connections and layer normalization to help speed up training (Figure 3a).\n",
    "\n",
    "<a name='ex-6'></a>    \n",
    "### Exercise 6 - DecoderLayer\n",
    "    \n",
    "Implement `DecoderLayer()` using the `call()` method\n",
    "    \n",
    "1. Block 1 is a multi-head attention layer with a residual connection, dropout layer, and look-ahead mask.\n",
    "2. Block 2 will take into account the output of the Encoder, so the multi-head attention layer will receive K and V from the encoder, and Q from the Block 1. You will then apply a dropout layer, layer normalization and a residual connection, just like you've done before. \n",
    "3. Finally, Block 3 is a feed forward neural network with dropout and normalization layers and a residual connection.\n",
    "    \n",
    "**Additional Hints:**\n",
    "* The first two blocks are fairly similar to the EncoderLayer except you will return `attention_scores` when computing self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "wEouNFvCzMeT"
   },
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION DecoderLayer\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The decoder layer is composed by two multi-head attention blocks, \n",
    "    one that takes the new input and uses self-attention, and the other \n",
    "    one that combines it with the output of the encoder, followed by a\n",
    "    fully connected block. \n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim)\n",
    "\n",
    "        self.mha2 = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim)\n",
    "\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
    "                                  fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.dropout3 = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            padding_mask -- Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            out3 -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            attn_weights_block1 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "            attn_weights_block2 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        # START CODE HERE\n",
    "        # enc_output.shape == (batch_size, input_seq_len, embedding_dim)\n",
    "        \n",
    "        # BLOCK 1\n",
    "        # calculate self-attention and return attention scores as attn_weights_block1 (~1 line)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x,look_ahead_mask, return_attention_scores=True)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        # apply dropout layer on the attention output (~1 line)\n",
    "        attn1 = self.dropout1(attn1, training = training)\n",
    "        \n",
    "        # apply layer normalization to the sum of the attention output and the input (~1 line)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        # BLOCK 2\n",
    "        # calculate self-attention using the Q from the first block and K and V from the encoder output.\n",
    "        # MultiHeadAttention's call takes input (Query, Value, Key, attention_mask, return_attention_scores, training)\n",
    "        # Return attention scores as attn_weights_block2 (~1 line)\n",
    "        attn2, attn_weights_block2 = self.mha2( out1,enc_output, enc_output, padding_mask, return_attention_scores=True)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        # apply dropout layer on the attention output (~1 line)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        \n",
    "        # apply layer normalization to the sum of the attention output and the output of the first block (~1 line)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, embedding_dim)\n",
    "        \n",
    "        #BLOCK 3\n",
    "        # pass the output of the second block through a ffn\n",
    "        ffn_output = self.ffn(out2) # (batch_size, target_seq_len, embedding_dim)\n",
    "        \n",
    "        # apply a dropout layer to the ffn output\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        \n",
    "        # apply layer normalization to the sum of the ffn output and the output of the second block\n",
    "        out3 =  self.layernorm3(ffn_output + out2) # (batch_size, target_seq_len, embedding_dim)\n",
    "        # END CODE HERE\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Wrong values in attn_w_b1. Check the call to self.mha1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(out[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.34323323\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.4689083\u001b[39m, \u001b[38;5;241m1.1092525\u001b[39m, \u001b[38;5;241m0.7028891\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong values in out when we mask the last word. Are you passing the padding_mask to the inner functions?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[92mAll tests passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mDecoderLayer_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDecoderLayer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36mDecoderLayer_test\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tf\u001b[38;5;241m.\u001b[39mshape(attn_w_b2)\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;241m==\u001b[39m shape1, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong shape. We expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tf\u001b[38;5;241m.\u001b[39mshape(out)\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;241m==\u001b[39m q\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong shape. We expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(attn_w_b1[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0.5271505\u001b[39m,  \u001b[38;5;241m0.47284946\u001b[39m, \u001b[38;5;241m0.\u001b[39m], atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong values in attn_w_b1. Check the call to self.mha1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(attn_w_b2[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0.33365652\u001b[39m, \u001b[38;5;241m0.32598493\u001b[39m, \u001b[38;5;241m0.34035856\u001b[39m]),  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong values in attn_w_b2. Check the call to self.mha2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(out[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0.04726627\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.6235218\u001b[39m, \u001b[38;5;241m1.0327158\u001b[39m, \u001b[38;5;241m0.54353976\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong values in out\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Wrong values in attn_w_b1. Check the call to self.mha1"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "def DecoderLayer_test(target):\n",
    "    \n",
    "    num_heads=8\n",
    "    tf.random.set_seed(10)\n",
    "    \n",
    "    decoderLayerq = target(\n",
    "        embedding_dim=4, \n",
    "        num_heads=num_heads,\n",
    "        fully_connected_dim=32, \n",
    "        dropout_rate=0.1, \n",
    "        layernorm_eps=1e-6)\n",
    "    \n",
    "    encoderq_output = tf.constant([[[-0.40172306,  0.11519244, -1.2322885,   1.5188192 ],\n",
    "                                   [ 0.4017268,   0.33922842, -1.6836855,   0.9427304 ],\n",
    "                                   [ 0.4685002,  -1.6252842,   0.09368491,  1.063099  ]]])\n",
    "    \n",
    "    q = np.array([[[1, 0, 1, 1], [0, 1, 1, 1], [1, 0, 0, 1]]]).astype(np.float32)\n",
    "    \n",
    "    look_ahead_mask = tf.constant([[1., 0., 0.],\n",
    "                       [1., 1., 0.],\n",
    "                       [1., 1., 1.]])\n",
    "    \n",
    "    padding_mask = None\n",
    "    out, attn_w_b1, attn_w_b2 = decoderLayerq(q, encoderq_output, True, look_ahead_mask, padding_mask)\n",
    "    \n",
    "    assert tf.is_tensor(attn_w_b1), \"Wrong type for attn_w_b1. Output must be a tensor\"\n",
    "    assert tf.is_tensor(attn_w_b2), \"Wrong type for attn_w_b2. Output must be a tensor\"\n",
    "    assert tf.is_tensor(out), \"Wrong type for out. Output must be a tensor\"\n",
    "    \n",
    "    shape1 = (q.shape[0], num_heads, q.shape[1], q.shape[1])\n",
    "    assert tuple(tf.shape(attn_w_b1).numpy()) == shape1, f\"Wrong shape. We expected {shape1}\"\n",
    "    assert tuple(tf.shape(attn_w_b2).numpy()) == shape1, f\"Wrong shape. We expected {shape1}\"\n",
    "    assert tuple(tf.shape(out).numpy()) == q.shape, f\"Wrong shape. We expected {q.shape}\"\n",
    "\n",
    "    assert np.allclose(attn_w_b1[0, 0, 1], [0.5271505,  0.47284946, 0.], atol=1e-2), \"Wrong values in attn_w_b1. Check the call to self.mha1\"\n",
    "    assert np.allclose(attn_w_b2[0, 0, 1], [0.33365652, 0.32598493, 0.34035856]),  \"Wrong values in attn_w_b2. Check the call to self.mha2\"\n",
    "    assert np.allclose(out[0, 0], [0.04726627, -1.6235218, 1.0327158, 0.54353976]), \"Wrong values in out\"\n",
    "    \n",
    "\n",
    "    # Now let's try a example with padding mask\n",
    "    padding_mask = np.array([[0, 0, 1]])\n",
    "    out, attn_w_b1, attn_w_b2 = decoderLayerq(q, encoderq_output, True, look_ahead_mask, padding_mask)\n",
    "\n",
    "    assert np.allclose(out[0, 0], [-0.34323323, -1.4689083, 1.1092525, 0.7028891]), \"Wrong values in out when we mask the last word. Are you passing the padding_mask to the inner functions?\"\n",
    "\n",
    "    print(\"\\033[92mAll tests passed\")\n",
    "    \n",
    "DecoderLayer_test(DecoderLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-2'></a> \n",
    "### 5.2 - Full Decoder\n",
    "You're almost there! Time to use your Decoder layer to build a full Transformer Decoder (Figure 3b). You will embedd your output and add positional encodings. You will then feed your encoded embeddings to a stack of Decoder layers. \n",
    "\n",
    "\n",
    "<img src=\"decoder.png\" alt=\"Encoder\" width=\"300\"/>\n",
    "<caption><center><font color='purple'><b>Figure 3b: Transformer Decoder</font></center></caption>\n",
    "\n",
    "<a name='ex-7'></a>     \n",
    "### Exercise 7 - Decoder\n",
    "\n",
    "Implement `Decoder()` using the `call()` method to embed your output, add positional encoding, and implement multiple decoder layers\n",
    " \n",
    "In this exercise, you will initialize your Decoder with an Embedding layer, positional encoding, and multiple DecoderLayers. Your `call()` method will perform the following steps: \n",
    "1. Pass your generated output through the Embedding layer.\n",
    "2. Scale your embedding by multiplying it by the square root of your embedding dimension. Remember to cast the embedding dimension to data type `tf.float32` before computing the square root.\n",
    "3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to your embedding.\n",
    "4. Pass the encoded embedding through a dropout layer, remembering to use the `training` parameter to set the model training mode. \n",
    "5. Pass the output of the dropout layer through the stack of Decoding layers using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "McS3by6k4pnP"
   },
   "outputs": [],
   "source": [
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION Decoder\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder is starts by passing the target input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    decoder Layers\n",
    "        \n",
    "    \"\"\" \n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(target_vocab_size, self.embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        \"\"\"\n",
    "        Forward  pass for the Decoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            padding_mask -- Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
    "            attention_weights - Dictionary of tensors containing all the attention weights\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        # START CODE HERE\n",
    "        # create word embeddings \n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, embedding_dim)\n",
    "        \n",
    "        # scale embeddings by multiplying by the square root of their dimension\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "        \n",
    "        # calculate positional encodings and add to word embedding\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # apply a dropout layer to x\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # use a for loop to pass x through a stack of decoder layers and update attention_weights (~4 lines total)\n",
    "        for i in range(self.num_layers):\n",
    "            # pass x and the encoder output through a stack of decoder layers and save the attention weights\n",
    "            # of block 1 and 2 (~1 line)\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            #update attention_weights dictionary with the attention weights of block 1 and block 2\n",
    "            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n",
    "        # END CODE HERE\n",
    "        \n",
    "        # x.shape == (batch_size, target_seq_len, embedding_dim)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-0.29326034  1.3223922  -1.418921    0.3897891 ], shape=(4,), dtype=float32)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Wrong values in outd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(att_weights[keys[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0.52145624\u001b[39m, \u001b[38;5;241m0.47854376\u001b[39m, \u001b[38;5;241m0.\u001b[39m]), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong values in att_weights[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeys[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[92mAll tests passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mDecoder_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDecoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36mDecoder_test\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(tf\u001b[38;5;241m.\u001b[39mshape(outd), tf\u001b[38;5;241m.\u001b[39mshape(encoderq_output)), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong shape. We expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mshape(encoderq_output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(outd[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(outd[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.2715261\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5606001\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.861783\u001b[39m, \u001b[38;5;241m1.69390933\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong values in outd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(att_weights\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(att_weights) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong type for att_weights[0]. Output must be a tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Wrong values in outd"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "def Decoder_test(target):\n",
    "    \n",
    "    tf.random.set_seed(10)\n",
    "        \n",
    "    num_layers=7\n",
    "    embedding_dim=4 \n",
    "    num_heads=3\n",
    "    fully_connected_dim=8\n",
    "    target_vocab_size=33\n",
    "    maximum_position_encoding=6\n",
    "    \n",
    "    x = np.array([[3, 2, 1], [2, 1, 0]])\n",
    "\n",
    "    \n",
    "    encoderq_output = tf.constant([[[-0.40172306,  0.11519244, -1.2322885,   1.5188192 ],\n",
    "                         [ 0.4017268,   0.33922842, -1.6836855,   0.9427304 ],\n",
    "                         [ 0.4685002,  -1.6252842,   0.09368491,  1.063099  ]],\n",
    "                        [[-0.3489219,   0.31335592, -1.3568854,   1.3924513 ],\n",
    "                         [-0.08761203, -0.1680029,  -1.2742313,   1.5298463 ],\n",
    "                         [ 0.2627198,  -1.6140151,   0.2212624 ,  1.130033  ]]])\n",
    "    \n",
    "    look_ahead_mask = tf.constant([[1., 0., 0.],\n",
    "                       [1., 1., 0.],\n",
    "                       [1., 1., 1.]])\n",
    "    \n",
    "    decoderk = Decoder(num_layers,\n",
    "                    embedding_dim, \n",
    "                    num_heads, \n",
    "                    fully_connected_dim,\n",
    "                    target_vocab_size,\n",
    "                    maximum_position_encoding)\n",
    "    outd, att_weights = decoderk(x, encoderq_output, False, look_ahead_mask, None)\n",
    "    \n",
    "    assert tf.is_tensor(outd), \"Wrong type for outd. It must be a dict\"\n",
    "    assert np.allclose(tf.shape(outd), tf.shape(encoderq_output)), f\"Wrong shape. We expected { tf.shape(encoderq_output)}\"\n",
    "    print(outd[1, 1])\n",
    "    assert np.allclose(outd[1, 1], [-0.2715261, -0.5606001, -0.861783, 1.69390933]), \"Wrong values in outd\"\n",
    "    \n",
    "    keys = list(att_weights.keys())\n",
    "    assert type(att_weights) == dict, \"Wrong type for att_weights[0]. Output must be a tensor\"\n",
    "    assert len(keys) == 2 * num_layers, f\"Wrong length for attention weights. It must be 2 x num_layers = {2*num_layers}\"\n",
    "    assert tf.is_tensor(att_weights[keys[0]]), f\"Wrong type for att_weights[{keys[0]}]. Output must be a tensor\"\n",
    "    shape1 = (x.shape[0], num_heads, x.shape[1], x.shape[1])\n",
    "    assert tuple(tf.shape(att_weights[keys[1]]).numpy()) == shape1, f\"Wrong shape. We expected {shape1}\" \n",
    "    assert np.allclose(att_weights[keys[0]][0, 0, 1], [0.52145624, 0.47854376, 0.]), f\"Wrong values in att_weights[{keys[0]}]\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed\")\n",
    "    \n",
    "Decoder_test(Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a> \n",
    "## 6 - Transformer\n",
    "\n",
    "Phew! This has been quite the assignment, and now you've made it to your last exercise of the Deep Learning Specialization. Congratulations! You've done all the hard work, now it's time to put it all together.  \n",
    "\n",
    "<img src=\"transformer.png\" alt=\"Transformer\" width=\"550\"/>\n",
    "<caption><center><font color='purple'><b>Figure 4: Transformer</font></center></caption>\n",
    "    \n",
    "The flow of data through the Transformer Architecture is as follows:\n",
    "* First your input passes through an Encoder, which is just repeated Encoder layers that you implemented:\n",
    "    - embedding and positional encoding of your input\n",
    "    - multi-head attention on your input\n",
    "    - feed forward neural network to help detect features\n",
    "* Then the predicted output passes through a Decoder, consisting of the decoder layers that you implemented:\n",
    "    - embedding and positional encoding of the output\n",
    "    - multi-head attention on your generated output\n",
    "    - multi-head attention with the Q from the first multi-head attention layer and the K and V from the Encoder\n",
    "    - a feed forward neural network to help detect features\n",
    "* Finally, after the Nth Decoder layer, two dense layers and a softmax are applied to generate prediction for the next output in your sequence.\n",
    "\n",
    "<a name='ex-8'></a> \n",
    "### Exercise 8 - Transformer\n",
    "\n",
    "Implement `Transformer()` using the `call()` method\n",
    "1. Pass the input through the Encoder with the appropiate mask.\n",
    "2. Pass the encoder output and the target through the Decoder with the appropiate mask.\n",
    "3. Apply a linear transformation and a softmax to get a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "QHymPmaj-2ba"
   },
   "outputs": [],
   "source": [
    "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION Transformer\n",
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Complete transformer with an Encoder and a Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n",
    "               target_vocab_size, max_positional_encoding_input,\n",
    "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers=num_layers,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               input_vocab_size=input_vocab_size,\n",
    "                               maximum_position_encoding=max_positional_encoding_input,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.decoder = Decoder(num_layers=num_layers, \n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               target_vocab_size=target_vocab_size, \n",
    "                               maximum_position_encoding=max_positional_encoding_target,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        self.final_layer = Dense(target_vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire Transformer\n",
    "        Arguments:\n",
    "            inp -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "            tar -- Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            enc_padding_mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "            look_ahead_mask -- Boolean mask for the target_input\n",
    "            padding_mask -- Boolean mask for the second multihead attention layer\n",
    "        Returns:\n",
    "            final_output -- Describe me\n",
    "            attention_weights - Dictionary of tensors containing all the attention weights for the decoder\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        # START CODE HERE\n",
    "        # call self.encoder with the appropriate arguments to get the encoder output\n",
    "        enc_output = self.encoder(inp,training,enc_padding_mask) # (batch_size, inp_seq_len, fully_connected_dim)\n",
    "        \n",
    "        # call self.decoder with the appropriate arguments to get the decoder output\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, fully_connected_dim)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        # pass decoder output through a linear layer and softmax (~2 lines)\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        # START CODE HERE\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0.02580714 0.05079436 0.04330779 0.01459846 0.01759386 0.04361821\n",
      " 0.02076122 0.01846705], shape=(8,), dtype=float32)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Wrong values in outd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(translation)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[92mAll tests passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m \u001b[43mTransformer_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTransformer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36mTransformer_test\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tf\u001b[38;5;241m.\u001b[39mshape(translation)\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;241m==\u001b[39m shape1, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong shape. We expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(translation[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m8\u001b[39m])\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(translation[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m8\u001b[39m],\n\u001b[1;32m     49\u001b[0m                    [[\u001b[38;5;241m0.02616475\u001b[39m, \u001b[38;5;241m0.02074359\u001b[39m, \u001b[38;5;241m0.01675757\u001b[39m, \n\u001b[1;32m     50\u001b[0m                      \u001b[38;5;241m0.025527\u001b[39m, \u001b[38;5;241m0.04473696\u001b[39m, \u001b[38;5;241m0.02171909\u001b[39m, \n\u001b[1;32m     51\u001b[0m                      \u001b[38;5;241m0.01542725\u001b[39m, \u001b[38;5;241m0.03658631\u001b[39m]]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong values in outd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(weights\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(weights) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong type for weights. It must be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Wrong values in outd"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "def Transformer_test(target):\n",
    "    \n",
    "    tf.random.set_seed(10)\n",
    "\n",
    "\n",
    "    num_layers = 6\n",
    "    embedding_dim = 4\n",
    "    num_heads = 4\n",
    "    fully_connected_dim = 8\n",
    "    input_vocab_size = 30\n",
    "    target_vocab_size = 35\n",
    "    max_positional_encoding_input = 5\n",
    "    max_positional_encoding_target = 6\n",
    "\n",
    "    trans = Transformer(num_layers, \n",
    "                        embedding_dim, \n",
    "                        num_heads, \n",
    "                        fully_connected_dim, \n",
    "                        input_vocab_size, \n",
    "                        target_vocab_size, \n",
    "                        max_positional_encoding_input,\n",
    "                        max_positional_encoding_target)\n",
    "    # 0 is the padding value\n",
    "    sentence_lang_a = np.array([[2, 1, 4, 3, 0]])\n",
    "    sentence_lang_b = np.array([[3, 2, 1, 0, 0]])\n",
    "\n",
    "    enc_padding_mask = np.array([[0, 0, 0, 0, 1]])\n",
    "    dec_padding_mask = np.array([[0, 0, 0, 1, 1]])\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(sentence_lang_a.shape[1])\n",
    "\n",
    "    translation, weights = trans(\n",
    "        sentence_lang_a,\n",
    "        sentence_lang_b,\n",
    "        True,\n",
    "        enc_padding_mask,\n",
    "        look_ahead_mask,\n",
    "        dec_padding_mask\n",
    "    )\n",
    "    \n",
    "    \n",
    "    assert tf.is_tensor(translation), \"Wrong type for translation. Output must be a tensor\"\n",
    "    shape1 = (sentence_lang_a.shape[0], max_positional_encoding_input, target_vocab_size)\n",
    "    assert tuple(tf.shape(translation).numpy()) == shape1, f\"Wrong shape. We expected {shape1}\"\n",
    "        \n",
    "    print(translation[0, 0, 0:8])\n",
    "    assert np.allclose(translation[0, 0, 0:8],\n",
    "                       [[0.02616475, 0.02074359, 0.01675757, \n",
    "                         0.025527, 0.04473696, 0.02171909, \n",
    "                         0.01542725, 0.03658631]]), \"Wrong values in outd\"\n",
    "    \n",
    "    keys = list(weights.keys())\n",
    "    assert type(weights) == dict, \"Wrong type for weights. It must be a dict\"\n",
    "    assert len(keys) == 2 * num_layers, f\"Wrong length for attention weights. It must be 2 x num_layers = {2*num_layers}\"\n",
    "    assert tf.is_tensor(weights[keys[0]]), f\"Wrong type for att_weights[{keys[0]}]. Output must be a tensor\"\n",
    "\n",
    "    shape1 = (sentence_lang_a.shape[0], num_heads, sentence_lang_a.shape[1], sentence_lang_a.shape[1])\n",
    "    assert tuple(tf.shape(weights[keys[1]]).numpy()) == shape1, f\"Wrong shape. We expected {shape1}\" \n",
    "    assert np.allclose(weights[keys[0]][0, 0, 1], [0.4992985, 0.5007015, 0., 0., 0.]), f\"Wrong values in weights[{keys[0]}]\"\n",
    "    \n",
    "    print(translation)\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed\")\n",
    "\n",
    "    \n",
    "Transformer_test(Transformer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've come to the end of the graded portion of the assignment. By now, you've: \n",
    "\n",
    "* Create positional encodings to capture sequential relationships in data\n",
    "* Calculate scaled dot-product self-attention with word embeddings\n",
    "* Implement masked multi-head attention\n",
    "* Build and train a Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    <b>What you should remember</b>:\n",
    "\n",
    "- The combination of self-attention and convolutional network layers allows of parallization of training and *faster training*.\n",
    "- Self-attention is calculated using the generated query Q, key K, and value V matrices.\n",
    "- Adding positional encoding to word embeddings is an effective way of include sequence information in self-attention calculations. \n",
    "- Multi-head attention can help detect multiple features in your sentence.\n",
    "- Masking stops the model from 'looking ahead' during training, or weighting zeroes too much when processing cropped sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have completed the Transformer assignment, make sure you check out the ungraded labs to apply the Transformer model to practical use cases such as Name Entity Recogntion (NER) and Question Answering (QA).  \n",
    "\n",
    "\n",
    "# Congratulations on finishing the Deep Learning Specialization!!!!!! 🎉\n",
    "\n",
    "This was the last graded assignment of the specialization. It is now time to celebrate all your hard work and dedication! \n",
    "\n",
    "<a name='7'></a> \n",
    "## 7 - References\n",
    "\n",
    "The Transformer algorithm was due to Vaswani et al. (2017). \n",
    "\n",
    "- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762) "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transformer Assignment - Subclass.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "DLSC5W4-1A"
   ]
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08b8eb7e54b0420692f439151110ea78": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a104536dc844996864bb00e07c351ff",
      "placeholder": "​",
      "style": "IPY_MODEL_77e7f5504f1a482980058bdd07d07d5e",
      "value": " 220/220 [14:41&lt;00:00,  4.01s/it]"
     }
    },
    "0a7b94269612418ab913737ada1a606f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f138da56eae488a85970020606e093d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7dd6278751c440baf1b80c19cdac901",
      "placeholder": "​",
      "style": "IPY_MODEL_32f739f4c4e742bc837232c8616c85c3",
      "value": " 442/442 [00:00&lt;00:00, 1.40kB/s]"
     }
    },
    "1171e92377e74bbd855dcf2cd49cd0fc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "156f653b1cf74c88a77e5ea66d59dddd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32f739f4c4e742bc837232c8616c85c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a104536dc844996864bb00e07c351ff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47460b36e9524825ab8228e5efae338e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bf260e4350dd4037bce3badc2c076b94",
       "IPY_MODEL_08b8eb7e54b0420692f439151110ea78"
      ],
      "layout": "IPY_MODEL_156f653b1cf74c88a77e5ea66d59dddd"
     }
    },
    "4ed12cd5a3a943ae862f026e20d0e9e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "52dbd62baaf04233a14a4278c9f5d24f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77e7f5504f1a482980058bdd07d07d5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8427a9c9ac78457cbdfffd41d76a467d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1171e92377e74bbd855dcf2cd49cd0fc",
      "placeholder": "​",
      "style": "IPY_MODEL_b4c3e99db1374b1b83b768dc1dc7c470",
      "value": " 363M/363M [00:06&lt;00:00, 56.4MB/s]"
     }
    },
    "84748d3f92494e839763a1ed7e3dc45e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a411c46cc2194664a08e1bc2eba903ff",
       "IPY_MODEL_0f138da56eae488a85970020606e093d"
      ],
      "layout": "IPY_MODEL_8b266584bf764912abb999040e1c4643"
     }
    },
    "8b266584bf764912abb999040e1c4643": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b8eb55aa69341148bc0fd72a5cc383d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a411c46cc2194664a08e1bc2eba903ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae662a275a7f443c868463b275454359",
      "max": 442,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a9d8813c381c43d5a17594a067f0c8c9",
      "value": 442
     }
    },
    "a9d8813c381c43d5a17594a067f0c8c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ae662a275a7f443c868463b275454359": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4c3e99db1374b1b83b768dc1dc7c470": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf260e4350dd4037bce3badc2c076b94": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a7b94269612418ab913737ada1a606f",
      "max": 220,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9b8eb55aa69341148bc0fd72a5cc383d",
      "value": 220
     }
    },
    "c13de54036014187a3c9e532e15112be": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2a1ddc5ec3d4e828d4abe111d0e3957": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52dbd62baaf04233a14a4278c9f5d24f",
      "max": 363423424,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4ed12cd5a3a943ae862f026e20d0e9e5",
      "value": 363423424
     }
    },
    "ded975cd21d640d7847ecaadd45387b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c2a1ddc5ec3d4e828d4abe111d0e3957",
       "IPY_MODEL_8427a9c9ac78457cbdfffd41d76a467d"
      ],
      "layout": "IPY_MODEL_c13de54036014187a3c9e532e15112be"
     }
    },
    "f7dd6278751c440baf1b80c19cdac901": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
